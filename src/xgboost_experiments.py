# -*- coding: utf-8 -*-
"""PleaseWork.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iRlh7bOcSdlvRJWakJwSgee3tu12VtHN
"""

# import gym
# from gym import spaces
# import numpy as np
# import tensorflow as tf
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Dense, Input
# from tensorflow.keras.optimizers import Adam
# import torch
# import torch.nn as nn
# import torch.optim as optim
# import pickle
# from torch.nn.utils import clip_grad_norm_
# import torch.nn.functional as F
# import gc
# from torch.utils.data import DataLoader, TensorDataset
# from torch.utils.checkpoint import checkpoint
# from sklearn.preprocessing import MinMaxScaler, StandardScaler



# with open('home_probabilities_2023.pickle', 'rb') as file:
#     home_probabilities_2023 = pickle.load(file)

# with open('home_odds_2023.pickle', 'rb') as file:
#     home_odds_2023 = pickle.load(file)

# with open('home_results_2023.pickle', 'rb') as file:
#     home_results_2023 = pickle.load(file)

# with open('home_probabilities_2022.pickle', 'rb') as file:
#     home_probabilities_2022 = pickle.load(file)

# with open('home_odds_2022.pickle', 'rb') as file:
#     home_odds_2022 = pickle.load(file)

# with open('home_results_2022.pickle', 'rb') as file:
#     home_results_2022 = pickle.load(file)

# with open('home_probabilities_2021.pickle', 'rb') as file:
#     home_probabilities_2021 = pickle.load(file)

# with open('home_odds_2021.pickle', 'rb') as file:
#     home_odds_2021 = pickle.load(file)

# with open('home_results_2021.pickle', 'rb') as file:
#     home_results_2021 = pickle.load(file)

# with open('home_probabilities_2020.pickle', 'rb') as file:
#     home_probabilities_2020 = pickle.load(file)

# with open('home_odds_2020.pickle', 'rb') as file:
#     home_odds_2020 = pickle.load(file)

# with open('home_results_2020.pickle', 'rb') as file:
#     home_results_2020 = pickle.load(file)

# with open('home_probabilities_2019.pickle', 'rb') as file:
#     home_probabilities_2019 = pickle.load(file)

# with open('home_odds_2019.pickle', 'rb') as file:
#     home_odds_2019 = pickle.load(file)

# with open('home_results_2019.pickle', 'rb') as file:
#     home_results_2019 = pickle.load(file)

# with open('home_probabilities_2018.pickle', 'rb') as file:
#     home_probabilities_2018 = pickle.load(file)

# with open('home_odds_2018.pickle', 'rb') as file:
#     home_odds_2018 = pickle.load(file)

# with open('home_results_2018.pickle', 'rb') as file:
#     home_results_2018 = pickle.load(file)

# with open('home_probabilities_2017.pickle', 'rb') as file:
#     home_probabilities_2017 = pickle.load(file)

# with open('home_odds_2017.pickle', 'rb') as file:
#     home_odds_2017 = pickle.load(file)

# with open('home_results_2017.pickle', 'rb') as file:
#     home_results_2017 = pickle.load(file)
# with open('home_probabilities_2016.pickle', 'rb') as file:
#     home_probabilities_2016 = pickle.load(file)

# with open('home_odds_2016.pickle', 'rb') as file:
#     home_odds_2016 = pickle.load(file)

# with open('home_results_2016.pickle', 'rb') as file:
#     home_results_2016 = pickle.load(file)

# with open('home_probabilities_2015.pickle', 'rb') as file:
#     home_probabilities_2015 = pickle.load(file)

# with open('home_odds_2015.pickle', 'rb') as file:
#     home_odds_2015 = pickle.load(file)

# with open('home_results_2015.pickle', 'rb') as file:
#     home_results_2015 = pickle.load(file)

# full_home_probabilities = np.concatenate((home_probabilities_2022, home_probabilities_2021, home_probabilities_2020, home_probabilities_2019, home_probabilities_2018, home_probabilities_2017, home_probabilities_2016, home_probabilities_2015))
# full_home_odds = np.concatenate((home_odds_2022, home_odds_2021, home_odds_2020, home_odds_2019, home_odds_2018, home_odds_2017, home_odds_2016, home_odds_2015))
# full_home_results = np.concatenate((home_results_2022, home_results_2021, home_results_2020, home_results_2019, home_results_2018, home_results_2017, home_results_2016, home_results_2015))

# full_odds_results = []
# for i in range(len(full_home_probabilities)):
#   if full_home_results[i] == 0:

#               if full_home_odds[i] < 0:  # Negative odds
#                   full_odds_results.append(100 / abs(full_home_odds[i]))
#               else:  # Positive odds
#                   full_odds_results.append(full_home_odds[i] / 100)
#   else:
#       full_odds_results.append(-1)

# full_odds_results = np.array(full_odds_results)
# # full_odds_results = np.array(full_odds_results)
# # print(full_odds_results[:50])
# # print(full_home_odds)

# full_home_probabilities.astype(np.float16)

# # print(f'full_odds_results: {full_odds_results}')
# # print(f'full_home_odds: {full_home_odds}')
# # print(f'max: {max(full_odds_results)}')


# scaler = StandardScaler()
# # full_odds_results = np.array(full_odds_results).reshape(-1, 1)  # Reshape to 2D array for a single feature
# full_home_odds = np.array(full_home_odds).reshape(-1, 1)

# # full_odds_results_scaled = scaler.fit_transform(full_odds_results)
# full_home_odds_scaled = scaler.fit_transform(full_home_odds)

# # print("full_odds_results:", full_odds_results)  # Flatten back to 1D if necessary
# # print("full_home_odds_scaled:", full_home_odds_scaled.flatten())
# # print(f'new max: {max(full_odds_results)}')

# model_predictions = [
#     (prob, odds, result, odds_results)
#     for prob, odds, result, odds_results in zip(full_home_probabilities, full_home_odds_scaled, full_home_results, full_odds_results)
# ]

# # print(f'model at 1: {model_predictions[0]}')
# # print(f'model at 2: {model_predictions[1]}')
# # print(f'model at 3: {model_predictions[2]}')

# del home_probabilities_2023
# del home_odds_2023
# del home_results_2023
# del home_probabilities_2022
# del home_odds_2022
# del home_results_2022
# del home_probabilities_2021
# del home_odds_2021
# del home_results_2021
# del home_probabilities_2020
# del home_odds_2020
# del home_results_2020
# del home_probabilities_2019
# del home_odds_2019
# del home_results_2019
# del home_probabilities_2018
# del home_odds_2018
# del home_results_2018
# del home_probabilities_2017
# del home_odds_2017
# del home_results_2017
# del home_probabilities_2016
# del home_odds_2016
# del home_results_2016
# del home_probabilities_2015
# del home_odds_2015
# del home_results_2015
# gc.collect()


# class BettingEnvironment:
#     def __init__(self, initial_balance, model_predictions):
#         self.initial_balance = initial_balance
#         self.balance = initial_balance
#         self.model_predictions = model_predictions  # Ensure this includes odds and results
#         self.idx = 0
#         self.total_profit = 0  # To track total profit
#         self.profit = 0  # To track profit
#         self.correct_predictions = 0  # To track correct predictions
#         self.bets = 0  # To track total number of bets made
#         self.total_correct_predictions = 0
#         self.total_bets = 0  # To track total number of bets made

#     def reset(self):
#         self.balance = self.initial_balance
#         self.idx = 0
#         self.profit = 0
#         self.correct_predictions = 0
#         self.bets = 0
#         return self.get_state()

#     def step(self, bet_amount):
#         bet_amount = min(torch.sigmoid(bet_amount).item() * self.balance, self.balance)
#         assert 0 <= bet_amount <= self.balance, "Bet amount must be within the available balance."

#         odds, result, bet_multiplier = self.model_predictions[self.idx][1], self.model_predictions[self.idx][2], self.model_predictions[self.idx][3]  # Assuming this structure
#         profit = self.resolve_bet(bet_amount, self.model_predictions[self.idx][3])
#         self.balance += profit
#         self.profit += profit
#         self.total_profit += profit  # Update total profit
#         if profit > 0:
#             self.correct_predictions += 1
#             self.total_correct_predictions += 1  # Update correct predictions if profit is positive
#         self.bets += 1
#         self.total_bets += 1  # Increment total bets
#         # Check if balance is depleted and apply a large negative reward
#         if profit == 0:
#           reward = -1
#           done = False
#         elif self.balance <= 0:
#             reward = -1  # Assign a large negative reward
#             done = True  # End the episode
#         else:
#             done = False
#             reward = profit  # Use profit as the reward



#         self.idx += 1
#         next_state = self.get_state()
#         done = done or self.idx >= len(self.model_predictions)

#         return next_state, reward, done, {}

#     def get_state(self):
#         if self.idx < len(self.model_predictions):
#             model_1_probability, odds = self.model_predictions[self.idx][:2]
#             return [self.balance, model_1_probability, odds]
#         else:
#             return [self.balance, 0, 0, 0]  # End of data

#     def resolve_bet(self, bet_amount, bet_multiplier):
#         return bet_amount * bet_multiplier
#         # if result == 0:
#         #     if odds < 0:  # Negative odds
#         #         return bet_amount * 100 / abs(odds)
#         #     else:  # Positive odds
#         #         return bet_amount * odds / 100
#         # else:
#         #     return -bet_amount


# class PolicyNetwork(nn.Module):
#     def __init__(self, input_dim, hidden_dim):
#         super(PolicyNetwork, self).__init__()
#         self.fc = nn.Sequential(
#             nn.Linear(input_dim, hidden_dim),
#             nn.ReLU(),
#             nn.Linear(hidden_dim, 2),  # Output both mean and log(std)
#         )

#     def forward(self, x):
#         # x = checkpoint(self.part1, x)
#         # x = checkpoint(self.part2, x)
#         # x = checkpoint(self.part3, x)
#         # x = checkpoint(self.part4, x)
#         # x = checkpoint(self.part5, x)
#         # x = checkpoint(self.part6, x)
#         # print(f'x: {x}')
#         if x.dim() == 1:
#             x = x.unsqueeze(0)  # Ensure there's a batch dimension
#         output = self.fc(x)
#         # print(f'output: {output}')


#         # Directly index assuming output is at least 2D [batch_size, 2]
#         mean = output[:, 0].unsqueeze(-1)  # Ensure shape is [batch_size, 1]
#         log_std = output[:, 1].unsqueeze(-1)  # Ensure shape is [batch_size, 1]
#         # print(f'mean: {mean}')
#         # print(f'log_std: {log_std}')

#         log_std = torch.clamp(log_std, min=-20, max=2)  # Example of clamping to prevent overflow
#         # print(torch.exp(log_std))
#         std = torch.exp(log_std) + 1e-5
#         return mean, std

#     # Define the evaluate method
#     def evaluate(self, states, actions):
#       log_probs_list = []
#       state_values_list = []
#       dist_entropy_list = []

#       # Ensure actions is a 2D tensor for indexing
#       actions = actions.unsqueeze(1)  # Add a dimension to actions to make it [N, 1]

#       for i in range(states.size(0)):
#           state = states[i].unsqueeze(0)  # Unsqueeze to add batch dimension

#           # Use the current state to compute mean and std
#           mean, std = self.forward(state)

#           # Create a Normal distribution and compute log_prob for the current action
#           dist = torch.distributions.Normal(mean, std)
#           log_prob = dist.log_prob(actions[i])  # Index into actions to get the current action
#           log_probs_list.append(log_prob)

#           # Assuming value_network is defined elsewhere and can handle batched inputs
#           state_value = value_network(state)
#           dist_entropy = dist.entropy()

#           state_values_list.append(state_value)
#           dist_entropy_list.append(dist_entropy)

#       # Concatenate the results
#       log_probs = torch.cat(log_probs_list, dim=0)
#       state_values = torch.cat(state_values_list, dim=0).squeeze(-1)  # Remove extra dimensions if any
#       dist_entropy = torch.cat(dist_entropy_list, dim=0)

#       return log_probs, state_values, dist_entropy






#     #     # for state, action in zip(states, actions):
#     #     print(f'states in EVALUATE: {states}')
#     #     print(f'actions IN EVALUATE: {actions}')
#     #     mean, std = self.forward(states)
#     #     print(f'mean: {mean}')
#     #     print(f'std: {std}')
#     #     dist = torch.distributions.Normal(mean, std)
#     #     log_probs = dist.log_prob(actions)
#     #     print(f'log_probs of evaluate function: {log_probs}')

#     #     # Integrate ValueNetwork here for state value computation
#     #     state_values = value_network(states)  # Directly use the ValueNetwork for state value predictions
#     #     print(f'yoooooooooo')
#     #     dist_entropy = dist.entropy()
#     #     print('looooooooooool')
#     #     return log_probs, state_values.squeeze(-1), dist_entropy  # Ensure state_values is correctly sized



# class ValueNetwork(nn.Module):
#     def __init__(self, input_dim, hidden_dim):
#         super(ValueNetwork, self).__init__()
#         self.fc = nn.Sequential(
#     nn.Linear(input_dim, hidden_dim),
#     nn.LeakyReLU(0.01),  # Leaky ReLU activation
#     nn.Linear(hidden_dim, hidden_dim),
#     nn.LeakyReLU(0.01),  # Leaky ReLU activation
#     nn.Linear(hidden_dim, 1),
# )



#     def forward(self, x):
#         # x = checkpoint(self.part1, x)
#         # x = checkpoint(self.part2, x)
#         # x = checkpoint(self.part3, x)
#         # x = checkpoint(self.part4, x)
#         # x = checkpoint(self.part5, x)
#         # x = checkpoint(self.part6, x)
#         # print('hiiiiiiiii')
#         return self.fc(x)


# def ppo_update(policy_network, value_network, optimizer, states, actions, rewards, old_log_probs, eps_clip=0.2, gamma=0.99, K_epochs=4, clip_param=1.0):
#     # Convert lists to tensors
#     # print(f'states: {states}')
#     # states = torch.stack(states)
#     states = torch.cat(states, dim=0)
#     # print(f'states after: {states}')
#     actions = torch.tensor(actions, dtype=torch.float)
#     rewards = torch.tensor(rewards, dtype=torch.float)
#     old_log_probs = torch.stack(old_log_probs)

#     # Calculate discounted rewards
#     discounted_rewards = []
#     R = 0
#     for reward in reversed(rewards):
#         R = reward + gamma * R
#         discounted_rewards.insert(0, R)
#     discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float)

#     # Normalize rewards
#     rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-7)

#     for _ in range(K_epochs):
#         log_probs, state_values, dist_entropy = policy_network.evaluate(states, actions)  # Now this includes state_values
#         advantages = rewards - state_values.detach()

#         # Calculate ratio (pi_theta / pi_theta_old)
#         torch.cuda.empty_cache()
#         log_probs = log_probs.to(dtype=torch.float32)
#         old_log_probs = old_log_probs.to(dtype=torch.float32)
#         old_log_probs = old_log_probs.squeeze()
#         # ratios = []
#         # for i in range(len(log_probs)):
#         #   ratios.append(torch.exp(log_probs[i] - old_log_probs[i]))
#         # ratios = torch.stack(ratios)
#         # print(f'ratios: {ratios}')
#         # print(f'len(log_probs): {len(log_probs)}')
#         # print(f'len(old_log_probs): {len(old_log_probs)}')
#         # print('plz dont crash')
#         # print(f'log_probs: {log_probs}')
#         # print(f'old_log_probs: {old_log_probs}')
#         # print(f'log_probs shape: {log_probs.size()}')
#         # print(f'old_log_probs shape: {old_log_probs.size()}')
#         # print(f'log_probs at 0: {log_probs[0]}')
#         # print(f'old_log_probs at 0: {old_log_probs[0]}')

#         ratios = torch.exp(log_probs - old_log_probs.detach()).to(dtype=torch.float32)
#         # print('it didnt crash')

#         # Calculate surrogate losses
#         surr1 = ratios * advantages
#         surr2 = torch.clamp(ratios, 1.0 - eps_clip, 1.0 + eps_clip) * advantages
#         policy_loss = -torch.min(surr1, surr2).mean()
#         # print(f'advantages: {advantages}')
#         # print(f'ratios: {ratios}')
#         # print(f'surr1: {surr1}')
#         # print(f'surr2: {surr2}')
#         # print(f'Policy Loss: {policy_loss}')



#         # Calculate value loss
#         value_loss = F.mse_loss(state_values.squeeze(-1), rewards)  # This returns a scalar value by default
#         # print(f'rewards: {rewards}')
#         # print(f'Value Loss: {value_loss}')

#         # print(f'dist_entropy: {dist_entropy}')

#         # Total loss
#         # loss = policy_loss + 0.5 * value_loss - 0.01 * dist_entropy
#         loss = policy_loss.mean() + 0.5 * value_loss.mean() - 0.01 * dist_entropy.mean()
#         # print(f'Loss: {loss}')

#         # loss.mean()

#         # Take gradient step
#         optimizer.zero_grad()
#         loss.backward(retain_graph=True)
#         # for param in list(policy_network.parameters()):
#         #   print(f'Policy Params: {param}')
#         # for param in list(value_network.parameters()):
#         #   print(f'Value Params: {param}')
#         # print(f'Value Parameters: {value_network.parameters()}')
#     clip_grad_norm_(policy_network.parameters(), clip_param)  # Gradient clipping
#     clip_grad_norm_(value_network.parameters(), clip_param)  # Gradient clipping for the value network as well
#     # print('last before nan')
#     optimizer.step()
#     # for param in list(policy_network.parameters()):
#     #   print(f'Policy Params: {param}')
#     # for param in list(value_network.parameters()):
#     #   print(f'Value Params: {param}')



# def sample_action(policy_network, state, current_balance):
#     # print(f'Current Balance: {current_balance}')
#     # state = np.array(state)
#     # If 'state' is a list of lists or a similar structure
#     # state_array = np.array(state).flatten()  # Flatten if necessary
#     # state_tensor = torch.tensor(state_array, dtype=torch.float32)
#     # Convert all parts of the state to tensors and ensure they are flat
#     state_tensors = [torch.tensor([state[0]], dtype=torch.float32),
#                     torch.tensor([state[1]], dtype=torch.float32),
#                     torch.tensor(state[2].flatten(), dtype=torch.float32)]  # Assuming state[2] is already flat

#     # Concatenate all tensors to form a single state tensor
#     state = torch.cat(state_tensors)

#     # print(state)
#     # state = torch.tensor(state, dtype=torch.float32)
#     mean, std = policy_network(state)
#     normal_dist = torch.distributions.Normal(mean, std)
#     action = normal_dist.sample()
#     log_prob = normal_dist.log_prob(action)
#     # print(f'log_prob of sample_action: {log_prob}')
#     # print(f'action: {action}
#     # action_scaled = torch.sigmoid(action)

#     # print(f"Action_1: {action_scaled.item()}")
#     # return action.item(), log_prob, normal_dist
#     return action, log_prob, normal_dist

# initial_balance = 10  # Starting balance
# env = BettingEnvironment(initial_balance, model_predictions)


# # Initialize networks and optimizer
# input_dim = 3  # Example: [model_1_prob, model_2_prob, odds]
# hidden_dim = 64
# output_dim = 1  # Continuous action space

# policy_network = PolicyNetwork(input_dim, hidden_dim)
# value_network = ValueNetwork(input_dim, hidden_dim)
# optimizer = optim.Adam(list(policy_network.parameters()) + list(value_network.parameters()), lr=.01)
# total_episodes = 100  # Define the total number of episodes
# max_steps = len(model_predictions)
# cumulative_profit = 0
# cumulative_correct_predictions = 0
# cumulative_total_bets = 0

# # # Assuming full_home_probabilities, full_home_odds, and full_home_results are numpy arrays
# # # Convert to tensors
# # data_tensors = torch.tensor(np.column_stack((full_home_probabilities, full_home_odds, full_home_results)), dtype=torch.float16)

# # # Create a dataset
# # dataset = TensorDataset(data_tensors[:, :3], data_tensors[:, 3:]) # Adjust indices based on your data structure

# # # Create a DataLoader
# # batch_size = 64 # Example batch size
# # dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)




# # Main training loop
# for episode in range(total_episodes):
#   # for batch_idx, (inputs, targets) in enumerate(dataloader):
#     states, actions, rewards, old_log_probs, state_values = [], [], [], [], []  # Initialize state_values here
#     state = env.reset()
#     current_balance = env.initial_balance
#     done = False
#     # print('next last step')
#     for t in range(max_steps):
#         # print(f'state: {state}')
#         action, log_prob, _ = sample_action(policy_network, state, current_balance)
#         # action_scaled = min(action * current_balance, current_balance) # Scale action to current balance
#         # action = min(action, current_balance)  # Clamp action to current balance

#         # Convert state to tensor for value network input
#         state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # Ensure state_tensor is batched
#         state_value = value_network(state_tensor)
#         # print(f'action: {action}')
#         next_state, reward, done, _ = env.step(action)
#         # current_balance += reward
#         current_balance = env.balance
#         # print(f'current_balance: {current_balance}')
#         # print(f'reward: {reward}')

#         # Store data for PPO update
#         states.append(state_tensor)
#         actions.append(torch.tensor(action))  # Ensure action is tensor for storage
#         rewards.append(reward)
#         old_log_probs.append(log_prob.unsqueeze(0))
#         # old_log_probs.append(log_prob)
#         state_values.append(state_value.squeeze(1))  # Remove extra dimension from state_value

#         state = next_state
#         if done:
#             print(f"Episode ended early at step {t+1}.")
#             break  # Exit the loop if the episode is done
#     # After each episode, log results
#     cumulative_profit += env.total_profit
#     cumulative_correct_predictions += env.correct_predictions
#     cumulative_total_bets += env.total_bets
#     episode_profit = env.profit
#     episode_accuracy = (env.correct_predictions / env.bets) * 100 if env.bets > 0 else 0

#     print(f"Episode {episode+1}: Profit = {episode_profit}, Accuracy = {episode_accuracy:.2f}%")
#     ppo_update(policy_network, value_network, optimizer, states, actions, rewards, old_log_probs, eps_clip=0.2, gamma=0.99, K_epochs=4, clip_param=1)
# # After all episodes, print cumulative results
# cumulative_accuracy = (cumulative_correct_predictions / cumulative_total_bets) * 100 if cumulative_total_bets > 0 else 0
# print(f"Cumulative Profit: {cumulative_profit}")
# print(f"Cumulative Accuracy: {cumulative_accuracy:.2f}%")