# -*- coding: utf-8 -*-
"""OtherSoccerNeuralnetwork

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oI5i4YmeRVC2eHeiPQhQYQvcD2q6NMWA
"""

# !pip install understat



import gym
from gym import spaces
import numpy as np

import torch
import torch.nn as nn
# import torch_xla
# import torch_xla.core.xla_model as xm
# import torch_xla.distributed.parallel_loader as pl
# import torch_xla.distributed.xla_multiprocessing as xmp

from torch.nn.utils.rnn import pad_sequence
from torch.distributions import Categorical
from understat import Understat
import asyncio
import aiohttp
import ssl
import certifi
import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.feature_selection import RFECV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import StackingClassifier
from sklearn.model_selection import learning_curve
from xgboost import plot_importance
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, precision_recall_curve
import time
from sklearn.model_selection import cross_val_predict
import statistics
import pickle
from joblib import dump, load
from torch.utils.data import DataLoader, TensorDataset
from transformers import BertTokenizer, BertForSequenceClassification
from transformers import BertModel
from transformers import AdamW
from torch.nn import TransformerEncoder, TransformerEncoderLayer, Dropout
from torch.optim.lr_scheduler import OneCycleLR
import copy
from torch.optim import Adam
import torch.nn.functional as F
from google.colab import files
from sklearn.inspection import permutation_importance
from sklearn.metrics import accuracy_score
import asyncio
import aiohttp
import ssl
import certifi
import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.feature_selection import RFECV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import StackingClassifier
from sklearn.model_selection import learning_curve
from xgboost import plot_importance
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, precision_recall_curve
import time
from sklearn.model_selection import cross_val_predict
import statistics
import pickle
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import LeakyReLU
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import Normalization
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling1D, Normalization
from sklearn.metrics import classification_report
from tensorflow.keras.models import load_model






async def main():
  if torch.cuda.is_available():
    device = torch.device("cuda")
    print(f"GPU available: {torch.cuda.get_device_name(0)}")
  else:
      device = torch.device("cpu")
      print("GPU not available, using CPU instead.")
  with open('reinforcement_vector_list.pickle', 'rb') as file:
      full_vector_list = pickle.load(file)

  with open('reinforcement_target_list.pickle', 'rb') as file:
        target_list = pickle.load(file)
  # # # print(f'target_list: {target_list}')

  # print(len(full_vector_list))
  # print(len(full_vector_list[0]))
  # print(full_vector_list[0])

  # Convert lists to numpy arrays
  full_vector_list = np.array(full_vector_list)
  target_list = np.array(target_list)

  # Standardize the vector list
  # scaler = StandardScaler()
  # full_vector_list = scaler.fit_transform(full_vector_list)

  # # Assuming target_list is a list of integer classes ranging from 0 to 2
  # y = to_categorical(target_list, num_classes=3)
  y = target_list

  # Split your data into training and testing sets
  X_train, X_test, y_train, y_test = train_test_split(full_vector_list, y, test_size=0.2, random_state=42)

  norm_layer = Normalization(axis=-1)  # Normalize across features
  norm_layer.adapt(X_train)
  # print(X_train.shape)
  # print(X_train.shape[1])

  model = Sequential([
      tf.keras.Input(shape=(X_train.shape[1])),
      norm_layer,  # Apply normalization as the first layer of the model
      Dense(774, activation='swish', kernel_regularizer=l2(0.001)),
      Dropout(0.5),
      Dense(512, activation='swish', kernel_regularizer=l2(0.001)),
      Dropout(0.5),
      Dense(512, activation='swish', kernel_regularizer=l2(0.001)),
      Dropout(0.5),
      # Add GlobalAveragePooling1D before the final Dense layer
      # GlobalAveragePooling1D(),
      # Dense(3, activation='softmax')  # Output layer for 3 classes
      Dense(1, activation='sigmoid')  # Output layer for 2 classes
  ])

  optimizer = Adam(learning_rate=0.001)
  # model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
  model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

  early_stopping = EarlyStopping(monitor='val_accuracy', patience=20, verbose=1, mode='max', restore_best_weights=True)

  model.fit(X_train, y_train, epochs=500, validation_data=(X_test, y_test), callbacks=[early_stopping])


  model.summary()
  loss, accuracy = model.evaluate(X_test, y_test)
  print(f"Test Loss: {loss}")
  print(f"Test Accuracy: {accuracy}")

  model.save('nn_model')  # Save as SavedModel



  y_pred = model.predict(X_test)
  y_pred_classes = (y_pred > 0.5).astype(int)
  y_true = y_test  # Assuming y_test is one-hot encoded; if not, this step is not needed

  # Generate the classification report
  # report = classification_report(y_true, y_pred_classes, target_names=['Class 0', 'Class 1', 'Class 2'])
  report = classification_report(y_true, y_pred_classes, target_names=['Negative', 'Positive'])
  print(report)

if __name__ == '__main__':
    # asyncio.run(main())
    await main()


    # (.001,1,.9921),()

# with open('reinforcement_test_vector_list.pickle', 'rb') as file:
#     test_vector_list = pickle.load(file)
# with open('reinforcement_test_target_list.pickle', 'rb') as file:
#     test_target_list = pickle.load(file)

# with open('reinforcement_game_info_list.pickle', 'rb') as file:
#     game_info_list = pickle.load(file)

# model = load_model('nn_model')
# norm_layer = model.layers[0]

# with open('reinforcement_vector_list.pickle', 'rb') as file:
#     full_vector_list = pickle.load(file)

# # with open('reinforcement_target_list.pickle', 'rb') as file:
# #       target_list = pickle.load(file)
# # # # # print(f'target_list: {target_list}')

# # # print(len(full_vector_list))
# # # print(len(full_vector_list[0]))
# # # print(full_vector_list[0])

# # # Convert lists to numpy arrays
# # full_vector_list = np.array(full_vector_list)
# # target_list = np.array(target_list)

# # # Standardize the vector list
# # # scaler = StandardScaler()
# # # full_vector_list = scaler.fit_transform(full_vector_list)

# # # # Assuming target_list is a list of integer classes ranging from 0 to 2
# # # y = to_categorical(target_list, num_classes=3)
# # y = target_list

# # # Split your data into training and testing sets
# # X_train, X_test, y_train, y_test = train_test_split(full_vector_list, y, test_size=0.2, random_state=42)

# # norm_layer = Normalization(axis=-1)  # Normalize across features
# # norm_layer.adapt(X_train)


# home_correct = 0
# home_incorrect = 0
# away_correct = 0
# away_incorrect = 0
# draw_correct = 0
# draw_incorrect = 0
# home_draw_correct = 0
# home_draw_incorrect = 0
# away_draw_correct = 0
# away_draw_incorrect = 0
# correct = 0
# total = 0
# new_actual = []
# # print('hi')
# # A = norm_layer(test_vector_list)
# # print('hi')
# # Data Preparation
# a = [game for game in full_vector_list]

# a = np.array(a)
# a = a.astype(np.int32)

# # Data Preprocessing - Normalizing
# norm_l = tf.keras.layers.Normalization(axis=-1)
# norm_l.adapt(a)
# A = norm_l(a)
# for idx, input  in enumerate(test_vector_list):
#   predictions = []
#   actual = []
#   # if kelly_criterion_calculator(money,odds_dictionary[game[0]],probability_of_winning)[2] < 0:
#   #     continue
#   # if odds_dictionary[game[0]] < -1000:
#       # continue
#   # a = game[2][:-1]
#   # b = game[2][-1]
#   # A = np.array(a)
#   # b = np.array(b)
#   # A = A.reshape(1, -1)
#   # norm_l.adapt(A)
#   # A = scaler.transform(A)
#   # predictions = best_model.predict(A)
#   # norm_l = tf.keras.layers.Normalization(axis=-1)
#   # norm_l.adapt(test_vector_list)
#   # A = norm_l(test_vector_list)
#   # A = norm_layer(input)

#   game = game_info_list[idx][0]
#   # print(game)
#   # print(game_info_list[idx][1])
#   # print(game_info_list[idx])
#   normalized_input = tf.reshape(A[idx], (1, -1))
#   prob = model.predict(normalized_input, verbose=0)
#   predictions = [0 if (prob[0][0] < .50) else 1]
#   # if game[1][0] == 0:
#   # if (results_dictionary[game[0]][0] == 'home_win') or (results_dictionary[game[0]][0] == 'draw'):
#   if (game_info_list[idx][1][2] == 'home_win') or (game_info_list[idx][1][2] == 'draw'):
#     actual.append(0)
#   else:
#     actual.append(1)


#   print(f"Game info: {game}, {game_info_list[idx][1]}")
#   print(f"Predicted class: {predictions[0]}, Right class: {actual[0]}, Probabilities: {prob}")

#   if predictions[0] == 0:
#     if actual[0] == 0:
#       home_draw_correct += 1
#       correct += 1
#     elif actual[0] == 1:
#       home_draw_incorrect += 1
#   elif predictions[0] == 1:
#     if actual[0] == 0:
#       away_draw_incorrect += 1
#     elif actual[0] == 1:
#       away_draw_correct += 1
#       correct += 1
#   total += 1
# print(f"Home/Draw Accuracy: {100 * home_draw_correct / (home_draw_correct + home_draw_incorrect)}%")
# print(f"Away/Draw Accuracy: {100 * away_draw_correct / (away_draw_correct + away_draw_incorrect)}%")
# print(f"Total Accuracy: {100 * correct / total}%")



# Load the saved model
model = load_model('nn_model')

# Load test data
with open('reinforcement_test_vector_list.pickle', 'rb') as file:
    test_vector_list = pickle.load(file)
with open('reinforcement_test_target_list.pickle', 'rb') as file:
    test_target_list = pickle.load(file)
with open('reinforcement_game_info_list.pickle', 'rb') as file:
    game_info_list = pickle.load(file)

# Load normalization layer settings if saved separately
# This step assumes you have a normalization layer object saved.
# If not, you'll need to adapt it from your training data again.

# Prepare the test data (assuming it's not normalized)
test_vector_list = np.array(test_vector_list)
# print(test_vector_list[0])
test_target_list = np.array(test_target_list)

# Normalize the test data using the same normalization layer used in training
# Assuming 'norm_layer' is your normalization layer
norm_layer = model.layers[0]  # Adjust this index if your normalization layer is at a different position
normalized_test_vectors = norm_layer(test_vector_list)
# print(normalized_test_vectors[0])

# Make predictions
predictions = model.predict(normalized_test_vectors)
predicted_classes = (predictions > 0.5).astype(int).flatten()

# Evaluate the model
print("Classification Report:")
print(classification_report(test_target_list, predicted_classes, target_names=['Negative', 'Positive']))

# Additional evaluations
correct_predictions = np.sum(predicted_classes == test_target_list)
total_predictions = len(test_target_list)
accuracy = correct_predictions / total_predictions
print(f"Accuracy: {accuracy*100:.2f}%")